# Configuration for the wunderio/silta-release subchart.
# see: https://github.com/wunderio/charts/blob/master/silta-release/values.yaml
silta-release:
  downscaler:
    enabled: true

# Main domain of the cluster.
# Subdomains of this domain will be created automatically for each environment.
clusterDomain: "silta.wdr.io"

# An optional human-readable label for the project, defaults to the repository name.
# This name is mainly used to create nice subdomains for each environment.
projectName: ""

# An optional human-readable label for the environment, defaults to the release name.
# We typically pass the branch name when we build dedicated environments per branch.
# This name is mainly used to create nice subdomains for each environment.
environmentName: ""

# The app label added to our Kubernetes resources.
app: frontend

# Domain names that will be mapped to this deployment.
# Example of exposing 2 additional domains for this deployment, each with its own certificate.
# exposeDomains:
#   example:
#     hostname: example.com
#   example2:
#     hostname: example1.com
#     # Reference to a key under `ingress`
#     ingress: gce
#     ssl:
#       enabled: true
#       issuer: letsencrypt-staging
#   example_www:
#     hostname: www.example.com
#   example_no_https:
#    hostname: insecure.example.com
#      ssl:
#        enabled: false
exposeDomains: {}

exposeDomainsDefaults:
  ingress: default
  ssl:
    enabled: true
    issuer: letsencrypt

# Settings for default site provided by this deployment
ssl:
  # Enable HTTPS for this deployment
  enabled:  true
  # Possible issuers: letsencrypt-staging, letsencrypt, selfsigned, custom
  issuer: letsencrypt
  # Only when certificate type is custom
  # ca: ""
  # key: ""
  # crt: ""

ingress:
  default:
    type: traefik
    tls: true
    redirect-https: true
    extraAnnotations:
      traefik.ingress.kubernetes.io/rate-limit: |
        extractorfunc: request.host
        rateset:
          default:
            period: 5s
            average: 150
            burst: 200
      nginx.ingress.kubernetes.io/limit-rps: "32"
      nginx.ingress.kubernetes.io/limit-rpm: "300"
      nginx.ingress.kubernetes.io/limit-burst-multiplier: "3"
      nginx.ingress.kubernetes.io/limit-connections: "20"
  gce:
    type: gce
    # The name of the reserved static IP address.
    # It is best to first reserve the IP address and then add it here.
    # staticIpAddressName: null
    # Custom ingress annotations
    # extraAnnotations:
    #   networking.gke.io/suppress-firewall-xpn-error: "true"

# Infrastructure related settings.
cluster:
  type: gke
  vpcNative: false

# Timezone to be used by the services that support it.
# List of timezones: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones#List (use the TZ identifier column)
# The default timezone for most container images is UTC.
timezone: ""

# backendConfig customizations for main service
backendConfig:
  securityPolicy:
    name: "silta-ingress"

# Add prefixes to the generated per-branch domains, to be used for projects that need to respond
# on multiple domains.
# domainPrefixes: ['en', 'fi']
domainPrefixes: []

# Domains by default are created following pattern branch.repository.cluster
# By enabling single subdomain it will be converted to branch-repository.cluster
singleSubdomain: false

# These variables are build-specific and should be passed via the --set parameter.
nginx:
  image: 'wunderio/silta-nginx:1.17-v1'

  # Requires "X-Proxy-Auth" header from upstream when value is non-empty string.
  x_proxy_auth: ""

  replicas: 1

  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 3
    metrics:
    - type: Resource
      resource:
        name: cpu
        targetAverageUtilization: 80

  # The Kubernetes resources for the nginx container.
  # These values are optimised for development environments.
  resources:
    requests:
      cpu: 1m
      memory: 10Mi

  loglevel: error

  # Header containing real IP address
  real_ip_header: X-Forwarded-For

  # Trust X-Forwarded-For from these hosts for getting external IP
  realipfrom:
    gke-internal: 10.0.0.0/8
    gce-health-check-1: 130.211.0.0/22
    gce-health-check-2: 35.191.0.0/16

  # Add IP addresses that should be excluded from basicauth.
  # Note that the key is only used for documentation purposes.
  noauthips:
    gke-internal: 10.0.0.0/8

  # Set of values to enable and use http basic authentication
  # It is implemented only for very basic protection (like web crawlers)
  basicauth:
    enabled: true

    # Define username and password
    credentials:
      username: silta
      password: demo

  # Security headers
  security_headers:
    X-Frame-Options: 'SAMEORIGIN'
    X-Content-Type-Options: 'nosniff'
    X-XSS-Protection: '"1; mode=block"'
    Referrer-Policy: '"no-referrer, strict-origin-when-cross-origin" always'

  # includeSubdomains should be used whenever possible, but before enabling it needs to be made sure there are no subdomains not using https:
  hsts_include_subdomains: ""
  #hsts_include_subdomains: " includeSubDomains;"
  #content_security_policy: "upgrade-insecure-requests; default-src https: data: 'unsafe-inline' 'unsafe-eval'; frame-ancestors 'self'; base-uri 'self'; object-src 'self'; connect-src wss: https:"

  # Extra configuration block in server context.
  serverExtraConfig: |

  # Extra configuration block in location context.
  locationExtraConfig: |

  # Extra configuration to pass to nginx as a file
  extraConfig: |

  # DEPRECATED. Please use serverExtraConfig instead
  extra_headers: {}

  # DEPRECATED. Please use locationExtraConfig instead
  extra_conditions: ""
    # |
    # # need this for node OPTIONS requests to work while site has bauth
    # add_header Access-Control-Allow-Methods 'GET,OPTIONS,PUT,DELETE,POST' always;
    # add_header Access-Control-Allow-Credentials 'true' always;
    # add_header Access-Control-Allow-Origin '$http_origin' always;
    # add_header Access-Control-Allow-Headers 'Authorization,DNT,User-Agent,Keep-Alive,Content-Type,accept,origin,X-Requested-With,Access-Control-Allow-Origin' always;
    # if ($request_method = OPTIONS ) {
    #   return 204;
    # }

services: {}
  # node:
  #   image: 'you need to pass in a value for frontend.image to your helm chart'
  #   port: 3000
  #   env: {}
  #   # When you use exposedRoute, nginx proxies request to service:[/exposedRoute] path.
  #   # This means, you have to implement this path in your application.
  #   #exposedRoute: '/'

  #   # How many instances of the Frontend pod should be in our Kubernetes deployment.
  #   # A single pod (the default value) is good for development environments to minimise resource usage.
  #   # Multiple pods make sense for high availability.
  #   replicas: 1

  #   # Use storage mountpoints (defined in the mounts section) for this service.
  #   mounts:
  #     - files

  #   # Enable autoscaling using HorizontalPodAutoscaler
  #   # see: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/
  #   autoscaling:
  #     enabled: false
  #     minReplicas: 1
  #     maxReplicas: 3
  #     metrics:
  #     - type: Resource
  #       resource:
  #         name: cpu
  #         targetAverageUtilization: 80

  #   resources:
  #     requests:
  #       cpu: 200m
  #       memory: 128Mi
  #     limits:
  #       memory: 512Mi

  #  # Post-install hook.
  #  # This is run every time a new environment (most commonly a first deployment for a new branch) is created
  #  postinstall:
  #    command: |
  #      your command goes here

  #  # Post-upgrade hook.
  #  # This is run every time a new release is deployed
  #  postupgrade:
  #    command: |
  #      your command goes here

  #  cron:
  #    example:
  #      command: echo "hello world"
  #      schedule: "~ 1 * * *"
  #      parallelism: 1
  #      nodeSelector:
  #        cloud.google.com/gke-nodepool: static-ip
  #      # Optionally override service resource requests
  #      resources:
  #        requests:
  #          cpu: 500m
  #  backup:
  #   # Extra commands for gathering data to be backed up.
  #   # Commands must store data at /backups/current
  #   command: |
  #     your command goes here

  #   nodeSelector:
  #     cloud.google.com/gke-nodepool: static-ip

# Configure the dynamically mounted volumes
mounts: {}
#  files:
#    enabled: true
#    storage: 1G
#    mountPath: /app/files
#    storageClassName: silta-shared
#    csiDriverName: csi-rclone
#    accessModes: ReadWriteMany

# Provide SSH access based on GitHub public keys and repository access.
# Note: Shell only works when the base image wunderio/silta-node is used!
shell:
  enabled: false
  gitAuth:
    apiToken: ''
    # Project's git repository URL
    repositoryUrl: ''
    outsideCollaborators: true
    keyserver:
      # Defaults to https://keys.[clusterDomain]/api/1/git-ssh-keys
      url: ''
      username: ''
      password: ''
  mount:
    storageClassName: silta-shared
    csiDriverName: csi-rclone
    accessModes: ReadWriteMany

backup:
  # Whether backups should be taken for the current environment.
  enabled: false

  # Cron schedule when backups should be taken, this is expected to take place daily.
  # ~ gets replaced with a random digit between 0 and 9 to prevent having all cron jobs at once.
  schedule: '~ 2 * * *'

  # How many days the backups will persist.
  retention: 14

  storage: 100Mi
  storageClassName: silta-shared
  csiDriverName: csi-rclone

  # Resources for the backup cron job.
  resources:
    requests:
      cpu: 100m
      memory: 122Mi
    limits:
      memory: 122Mi

# Provide defaults for all services.
# Note that only keys used here can be overridden.
serviceDefaults:
  port: 3000
  resources:
    requests:
      cpu: 2m
      memory: 64Mi
    limits:
      memory: 256Mi
  # https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
  strategy:
    type: RollingUpdate
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 3
    metrics:
    - type: Resource
      resource:
        name: cpu
        targetAverageUtilization: 80

# Provide defaults for all cron jobs. This will override serviceDefaults when defined.
cronJobDefaults:
  # resources:
  #   requests:
  #     cpu: 100m
  #     memory: 122Mi
  #   limits:
  #     memory: 122Mi
  # nodeSelector:
  #   cloud.google.com/gke-nodepool: static-ip

# Override the default values of the MariaDB subchart.
# These settings are optimised for development environments.
# see: https://github.com/bitnami/charts/blob/master/bitnami/mariadb/values.yaml
mariadb:
  enabled: false
  image:
    # https://hub.docker.com/r/bitnami/mariadb/tags
    tag: 10.6.15-debian-11-r24
  replication:
    enabled: false
  db:
    name: frontend
    user: frontend
  master:
    persistence:
      # Database storage disk space allocation
      # Request assistance from ops team after changing this on existing deployment.
      size: 1G
    resources:
      requests:
        cpu: 25m
        memory: 366Mi
      limits:
        memory: 488Mi
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          preference:
            matchExpressions:
            - key: cloud.google.com/gke-nodepool
              operator: NotIn
              values:
              - static-ip
  enableServiceLinks: false

varnish:
  enabled: false
  resources:
    requests:
      cpu: 25m
      memory: 32Mi
  image: wunderio/silta-varnish
  imageTag: 7-v1
  imagePullPolicy: IfNotPresent
  # https://varnish-cache.org/docs/7.4/reference/varnishd.html#storage-backend
  storageBackend: 'file,/var/lib/varnish/varnish_storage.bin,512M'
  # https://varnish-cache.org/docs/7.4/reference/varnishd.html#list-of-parameters
  extraParams: ""
  # Custom caching code for vcl_recv subroutine.
  vcl_recv: |
    # Deny special pages to bots
    if (req.url ~ "^/(?:user|admin|cart|checkout|logout|abuse|flag|.*\?rate=)" && req.http.user-agent ~ "(?:crawl|goog|yahoo|spider|bot|Yandex|bing|tracker|click|parser|ltx71|urllib)") {
      return (synth( 403, "Forbidden"));
    }

    # Do not cache anything
    return (pass);
    
  # Custom caching code for vcl_backend_response subroutine.
  vcl_backend_response: |

  # Custom html code for 50x pages and backend failures.
  status_500_html: ""
  # https://varnish-cache.org/docs/7.4/reference/vcl-backend.html
  backend_config: |
    .max_connections = 300;
    .probe = {
      .request =
          "HEAD / HTTP/1.1"
          "User-Agent: Varnish"
          "Connection: close"
          "Host: localhost";
      .interval  = 5s;
      .timeout   = 5s;
      .window    = 5;
      .threshold = 3;
    }
    .first_byte_timeout     = 300s;
    .connect_timeout        = 10s;
    .between_bytes_timeout  = 10s;

elasticsearch:
  enabled: false

  # The elasticsearch version to use.
  # It's a good idea to tag this in your silta.yml
  imageTag: 7.16.2

  replicas: 1
  minimumMasterNodes: 1
  maxUnavailable: 0
  clusterHealthCheckParams: 'wait_for_status=yellow&timeout=1s'

  createCert: false
  protocol: http
  extraEnvs:
    - name: xpack.security.enabled
      value: "false"

  # This value should be slightly less than 50% of the requested memory.
  esJavaOpts: -Xmx220m -Xms220m
  xpack:
    enabled: false
  volumeClaimTemplate:
    resources:
      requests:
        storage: 1Gi

  resources:
    requests:
      cpu: 200m
      memory: 640Mi
    limits:
      memory: 1Gi

  nodeAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      preference:
        matchExpressions:
        - key: cloud.google.com/gke-nodepool
          operator: NotIn
          values:
          - static-ip

# These settings are not optimized. To see which overrides are available see:
# https://github.com/bitnami/charts/blob/master/bitnami/mongodb/values.yaml
mongodb:
  enabled: false
  image:
    tag: 5.0.3

  # Use a low default to prevent unnecessary storage use.
  persistence:
    size: 1Gi

# Posgresql overrides
# see: https://github.com/bitnami/charts/blob/c3dc56f3679650f1012e497b8a5e71d94dac163e/bitnami/postgresql/values.yaml
postgresql:
  enabled: false
  # Use a low default to prevent unnecessary storage use.
  persistence:
    size: 1Gi

rabbitmq:
  enabled: false

  # Use a low default to prevent unnecessary storage use.
  persistence:
    size: 1Gi

# Add following lines to your node.Dockerfile when enabling instana monitoring
#   RUN npm install -g @instana/collector
#   ENV NODE_OPTIONS="--require /usr/local/lib/node_modules/@instana/collector/src/immediate"
instana:
  enabled: false

# Fastly Signal Sciences support
# https://docs.fastly.com/signalsciences/
signalsciences:
  enabled: false
  accesskeyid: ""
  secretaccesskey: ""
  image: signalsciences/sigsci-agent
  imageTag: 4.40.0
  # sidecar container resources
  resources:
    requests:
      cpu: 20m
      memory: 40Mi
    limits:
      cpu: 40m
      memory: 300Mi

# Mailhog service overrides
# see: https://github.com/codecentric/helm-charts/blob/master/charts/mailhog/values.yaml
mailhog:
  enabled: false
  image:
    # Set image repository to "mailhog/mailhog" to reenable logging.
    repository: wunderio/silta-mailhog
    # Set image tag to "" to reenable logging.
    tag: v1
  resources:
    requests:
      cpu: 1m
      memory: 10M
